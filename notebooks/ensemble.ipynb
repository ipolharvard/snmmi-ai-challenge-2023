{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ospkg.constants import RESULTS_DIR\n",
    "from ospkg.ensemble_snmmi import aggregate_results, build_dataframe, compute_c_indices, ensemble\n",
    "from ospkg.utils import apply_alpha_correction, compute_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1: MEAN SQUARED ERROR BETWEEN PREDICTED AND OBSERVED PFS IN MONTHS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 SELECTION OF MODELS: ENSEMBLE OF BEST PERFORMING MODELS.\n",
    "\n",
    "In this section,\n",
    "\n",
    "- We rank the models according to their average MSE over the 5 validation folds. \n",
    "- We create ensembles of increasing number of models by combining the TOP_1 (single models), TOP_2, TOP_3, ..., TOP_N best performing models (i.e., models with lowest MSE). \n",
    "- For each ensemble, we compute the ensemble's prediction (estimated PFS) by averaging the predictions of each of the model members.\n",
    "- We calculate the MSE for each ensemble, and select the ensemble with the lowest MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = \"mse-avg\"\n",
    "df_val = build_dataframe(data_dir=RESULTS_DIR, criterion=criterion)\n",
    "df_val.to_csv(\"tmp_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the search for best ensemble for the first task of the challenge.\n",
    "# We've noticed that performance does not change much after we add 10+ models. So we stop at 16.\n",
    "\n",
    "TOP_N = 25\n",
    "aggregation_method = \"mean\"  # mean, median, no-tail\n",
    "\n",
    "# Parameters for plot\n",
    "k = int(np.sqrt(TOP_N))\n",
    "plt.figure(figsize=(k * 4, k * 4))\n",
    "\n",
    "# Loop and create increasing number of ensemble models!\n",
    "best_ensemble_n = 1  # TOP_1 initially.\n",
    "best_MSE = np.inf\n",
    "\n",
    "for n_ in range(1, TOP_N + 1):\n",
    "\n",
    "    df_ensemble_mse = ensemble(df=df_val, top_n=n_, criterion=\"mse-avg\", fold=-1)\n",
    "\n",
    "    pred, true, event = aggregate_results(df=df_ensemble_mse, method=aggregation_method)\n",
    "\n",
    "    error = pred - true\n",
    "    mse = np.mean(error**2)\n",
    "\n",
    "    plt.subplot(k, k, n_)\n",
    "\n",
    "    # Get Errors for Censored vs Uncensored\n",
    "    err_censored_hist, bins = np.histogram(error[event == 0.0], bins=np.arange(-100, 100, 4))\n",
    "    plt.step(bins[1:], err_censored_hist, \"r-\", linewidth=1, label=\"Censored\")\n",
    "\n",
    "    err_censored_hist, bins = np.histogram(error[event == 1.0], bins=np.arange(-100, 100, 4))\n",
    "    plt.step(bins[1:], err_censored_hist, \"g-\", linewidth=1, label=\"Uncensored\")\n",
    "\n",
    "    plt.title(f\"TOP_{n_} - MSE = {mse: 1.0f}\")\n",
    "\n",
    "    if n_ > k * (k - 1):\n",
    "        plt.xlabel(\"(PRED - TRUE) [Months]\")\n",
    "    if (n_ - 1) % k == 0:\n",
    "        plt.ylabel(\"Counts\")\n",
    "\n",
    "    if n_ == 1:\n",
    "        plt.legend()\n",
    "\n",
    "    # UPDATE BEST ENSEMBLE FOUND!\n",
    "    if mse < best_MSE:\n",
    "        best_ensemble_n = n_\n",
    "        best_MSE = mse\n",
    "\n",
    "plt.savefig(f\"Ensemble_top_N_{aggregation_method}_for_{criterion}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best ensemble:\n",
    "print(\n",
    "    f\"The best ensemble found for {criterion} consists of the top {best_ensemble_n} models, and yields an average MSE = {best_MSE: 1.1f} months ** 2\"\n",
    ")\n",
    "\n",
    "best_ensemble_df = ensemble(\n",
    "    df=df_val, top_n=best_ensemble_n, criterion=criterion, fold=-1, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, true, event = aggregate_results(df=best_ensemble_df, method=aggregation_method)\n",
    "error = pred - true\n",
    "plt.plot(pred[event == 0], error[event == 0], \"ro\", label=\"Censored\")\n",
    "plt.plot(pred[event == 1], error[event == 1], \"go\", label=\"Uncensored\")\n",
    "plt.title(\n",
    "    f\"Ensemble: TOP {best_ensemble_n}; \\n criterion: {criterion}; aggregation: {aggregation_method}\"\n",
    ")\n",
    "plt.xlabel(\"Predicted Time to Event [Months]\")\n",
    "plt.ylabel(\"(PRED - TRUE)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_opt, th, al = compute_alpha(pred, true)\n",
    "\n",
    "print(th, al, ((pred - true) ** 2).sum() / len(pred), ((pred_opt - true) ** 2).sum() / len(pred))\n",
    "\n",
    "error = pred_opt - true\n",
    "plt.plot(pred_opt[event == 0], error[event == 0], \"ro\", label=\"Censored\")\n",
    "plt.plot(pred_opt[event == 1], error[event == 1], \"go\", label=\"Uncensored\")\n",
    "plt.title(\n",
    "    f\"Ensemble: TOP {best_ensemble_n} with ALPHA correction; \\n criterion: {criterion}; aggregation: {aggregation_method}\"\n",
    ")\n",
    "plt.xlabel(\"Predicted Time to Event [Months]\")\n",
    "plt.ylabel(\"(PRED - TRUE)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >>> Generate Results over TEST set, for TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Test Set Dataframe\n",
    "df_test = build_dataframe(data_dir=RESULTS_DIR, test=True, criterion=\"mse-avg\")\n",
    "\n",
    "# Reminde me of the selected Ensemble:\n",
    "print(\"The Selected Ensemble is composed of:\")\n",
    "for selected_ in best_ensemble_df[\"Model\"].unique():\n",
    "    print(f\" >> {selected_}\")\n",
    "\n",
    "# Retrieve these models from TEST set dataframe\n",
    "df_test_selected = df_test[df_test[\"Model\"].isin(best_ensemble_df[\"Model\"].unique())]\n",
    "\n",
    "if (best_ensemble_df[\"Model\"].unique() != df_test_selected[\"Model\"].unique()).all():\n",
    "    raise AssertionError(\"Couldn't find all the models in the TEST set dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the results:\n",
    "if aggregation_method == \"mean\":\n",
    "    pred_continuous_pfs = df_test_selected.groupby(by=\"PatientID\")[\"PFS_pred\"].mean()\n",
    "else:\n",
    "    raise ValueError(\"Are you sure? Mean aggregation seemed to be the best so far. PLease review\")\n",
    "\n",
    "# Apply correction:\n",
    "pred_submit = pd.DataFrame(pred_continuous_pfs)\n",
    "pred_submit[\"Predicted PFS\"] = pred_submit.apply(\n",
    "    lambda x: apply_alpha_correction(x[\"PFS_pred\"], th, al), axis=1\n",
    ")\n",
    "\n",
    "pred_submit = pred_submit.drop(labels=\"PFS_pred\", axis=1)\n",
    "pred_submit.index = pred_submit.index.astype(int)\n",
    "pred_submit = pred_submit.sort_values(by=\"PatientID\", ascending=True)\n",
    "pred_submit.to_csv(\"PFSContinuousOutcome.csv\")\n",
    "\n",
    "# And plot the distributions and compare against Validation\n",
    "hist_val, bins_ = np.histogram(pred_opt, bins=30)\n",
    "hist_test, _ = np.histogram(pred_submit[\"Predicted PFS\"], bins=bins_)\n",
    "\n",
    "plt.step(bins_[1:], hist_val / np.sum(hist_val), \"r\", label=\"Validation\")\n",
    "plt.step(bins_[1:], hist_test / np.sum(hist_test), \"b\", label=\"Test\")\n",
    "plt.title(\"TASK 1\")\n",
    "plt.xlabel(\"Estimated Time to Event [Months]\")\n",
    "plt.ylabel(\"Normalized Counts\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 SELECTION OF MODELS: ENSEMBLE OF RANDOM MODELS. [DISABLED FOR NOW AS WE DO NOT WANT TO OVERFIT]\n",
    "\n",
    "In this section,\n",
    "\n",
    "- We create ensembles by sampling up to 5 models, at random, several times. \n",
    "- For each ensemble, we calculate MSE.\n",
    "- We select the ensemble with the lowest MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False, \"We are NOT running this. Very high chance of overfitting.\"\n",
    "\n",
    "num_samples = 10000\n",
    "TOP_N = 0\n",
    "\n",
    "top_1 = ensemble(df=df_val, top_n=1, criterion=criterion, fold=-1)  # TOP_1 initially.\n",
    "best_ensemble_df = top_1\n",
    "best_MSE = best_ensemble_df[\"mse-avg\"].mean()\n",
    "\n",
    "print(f\"Current MSE to beat: {best_MSE: 1.1f}\")\n",
    "\n",
    "for i in range(num_samples):\n",
    "\n",
    "    num_models = np.random.choice([1, 2, 3, 4], 1)\n",
    "    df_ensemble = ensemble(\n",
    "        df=df_val, top_n=TOP_N, criterion=criterion, fold=-1, num_random=num_models[0]\n",
    "    )\n",
    "\n",
    "    # Always include Top_1?\n",
    "    df_ensemble = pd.concat([top_1, df_ensemble])\n",
    "\n",
    "    pred, true, event = aggregate_results(df=df_ensemble, method=aggregation_method)\n",
    "\n",
    "    mse = np.mean((pred - true) ** 2)\n",
    "\n",
    "    if mse < best_MSE:\n",
    "        print(f\"Found lower MSE: {mse: 1.1f}, sample = {i} /{num_samples}\")\n",
    "        best_ensemble_df = df_ensemble.copy()\n",
    "        best_MSE = mse\n",
    "\n",
    "# The best ensemble:\n",
    "print(\n",
    "    f\"The best ensemble found for {criterion} consists of the {best_ensemble_df['Model'].unique()} models, and yields an average MSE = {best_MSE: 1.1f} months ** 2\"\n",
    ")\n",
    "\n",
    "pred, true, event = aggregate_results(df=best_ensemble_df, method=aggregation_method)\n",
    "error = pred - true\n",
    "plt.plot(pred[event == 0], error[event == 0], \"ro\", label=\"Censored\")\n",
    "plt.plot(pred[event == 1], error[event == 1], \"go\", label=\"Uncensored\")\n",
    "plt.title(\n",
    "    f\"Ensemble: Best Random Models; \\n criterion: {criterion}; aggregation: {aggregation_method}\"\n",
    ")\n",
    "plt.xlabel(\"Predicted Time to Event [Months]\")\n",
    "plt.ylabel(\"(PRED - TRUE)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply alpha correction\n",
    "For non-mse error (most survival methods) we correct the estimates of pfs because larger PFS tend to be censored and by this the \n",
    "actual estimate will tend to overestimate the actual pfs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_opt, th, al = compute_alpha(pred, true)\n",
    "\n",
    "print(\n",
    "    th, al, ((pred - true) ** 2).sum() / len(pred), ((pred_opt - true) ** 2).sum() / len(pred_opt)\n",
    ")\n",
    "\n",
    "error = pred_opt - true\n",
    "plt.plot(pred_opt[event == 0], error[event == 0], \"ro\", label=\"Censored\")\n",
    "plt.plot(pred_opt[event == 1], error[event == 1], \"go\", label=\"Uncensored\")\n",
    "plt.title(\n",
    "    f\"Ensemble: Best Random Models; \\n criterion: {criterion}; aggregation: {aggregation_method}\"\n",
    ")\n",
    "plt.xlabel(\"Predicted Time to Event [Months]\")\n",
    "plt.ylabel(\"(PRED - TRUE)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# >>> GENERATE RESULTS OVER TEST, FOR TASK 1: RISKY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Test Set Dataframe\n",
    "df_test = build_dataframe(data_dir=RESULTS_DIR, test=True, criterion=\"mse-avg\")\n",
    "\n",
    "# Reminde me of the selected Ensemble:\n",
    "print(\"The Selected Ensemble is composed of:\")\n",
    "for selected_ in best_ensemble_df[\"Model\"].unique():\n",
    "    print(f\" >> {selected_}\")\n",
    "\n",
    "# Retrieve these models from TEST set dataframe\n",
    "df_test_selected = df_test[df_test[\"Model\"].isin(best_ensemble_df[\"Model\"].unique())]\n",
    "\n",
    "if (best_ensemble_df[\"Model\"].unique() != df_test_selected[\"Model\"].unique()).all():\n",
    "    raise AssertionError(\"Couldn't find all the models in the TEST set dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the results:\n",
    "if aggregation_method == \"mean\":\n",
    "    pred_continuous_pfs = df_test_selected.groupby(by=\"PatientID\")[\"PFS_pred\"].mean()\n",
    "else:\n",
    "    raise ValueError(\"Are you sure? Mean aggregation seemed to be the best so far. PLease review\")\n",
    "\n",
    "# Apply correction:\n",
    "pred_submit = pd.DataFrame(pred_continuous_pfs)\n",
    "pred_submit[\"Predicted PFS\"] = pred_submit.apply(\n",
    "    lambda x: apply_alpha_correction(x[\"PFS_pred\"], th, al), axis=1\n",
    ")\n",
    "\n",
    "pred_submit = pred_submit.drop(labels=\"PFS_pred\", axis=1)\n",
    "pred_submit.index = pred_submit.index.astype(int)\n",
    "pred_submit = pred_submit.sort_values(by=\"PatientID\", ascending=True)\n",
    "pred_submit.to_csv(\"PFSContinuousOutcomeRISKY.csv\")\n",
    "\n",
    "# And plot the distributions and compare against Validation\n",
    "hist_val, bins_ = np.histogram(pred_opt, bins=30)\n",
    "hist_test, _ = np.histogram(pred_submit[\"Predicted PFS\"], bins=bins_)\n",
    "\n",
    "plt.step(bins_[1:], hist_val / np.sum(hist_val), \"r\", label=\"Validation\")\n",
    "plt.step(bins_[1:], hist_test / np.sum(hist_test), \"b\", label=\"Test\")\n",
    "plt.title(\"TASK 1: RISKY\")\n",
    "plt.xlabel(\"Estimated Time to Event [Months]\")\n",
    "plt.ylabel(\"Normalized Counts\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 2: AVERAGE C-INDEX FOR 1, 2 and 3 YEARS PFS TO ASSESS THE BEST CLASSIFLYING MODEL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 SELECTION OF MODELS: ENSEMBLE OF TOP_N PERFORMING MODELS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload DataFrame, exclude models with invalid outputs for c-index.\n",
    "criterion = \"c-index-123-avg\"\n",
    "df_val = build_dataframe(data_dir=RESULTS_DIR, criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the search.\n",
    "criterion = \"c-index-123-avg\"\n",
    "TOP_N = 40\n",
    "aggregation_method = \"mean\"\n",
    "\n",
    "best_c = 0\n",
    "best_ensemble_df = pd.DataFrame()\n",
    "\n",
    "for n_ in range(1, TOP_N + 1):\n",
    "\n",
    "    df_ensemble_c123 = ensemble(df=df_val, top_n=n_, criterion=criterion, fold=-1)\n",
    "\n",
    "    prob_1, true_1, event_1 = aggregate_results(\n",
    "        df=df_ensemble_c123, method=aggregation_method, task=\"SurvProb1\"\n",
    "    )\n",
    "    prob_2, true_2, event_2 = aggregate_results(\n",
    "        df=df_ensemble_c123, method=aggregation_method, task=\"SurvProb2\"\n",
    "    )\n",
    "    prob_3, true_3, event_3 = aggregate_results(\n",
    "        df=df_ensemble_c123, method=aggregation_method, task=\"SurvProb3\"\n",
    "    )\n",
    "\n",
    "    # Sanity checks\n",
    "    assert (true_1 == true_2).all()\n",
    "    assert (event_1 == event_2).all()\n",
    "    assert (true_1 == true_3).all()\n",
    "    assert (event_1 == event_3).all()\n",
    "\n",
    "    # Compute C-indices\n",
    "    c_1, c_2, c_3 = compute_c_indices(\n",
    "        dura=true_1.to_list(),\n",
    "        ev=event_1.to_list(),\n",
    "        p1=prob_1.to_list(),\n",
    "        p2=prob_2.to_list(),\n",
    "        p3=prob_3.to_list(),\n",
    "    )\n",
    "\n",
    "    c_123 = (c_1 + c_2 + c_3) / 3\n",
    "\n",
    "    if c_123 > best_c:\n",
    "        best_c = c_123\n",
    "        best_ensemble_df = df_ensemble_c123.copy()\n",
    "\n",
    "# The best ensemble:\n",
    "print(\n",
    "    f\"The best ensemble found for {criterion} consists of the {best_ensemble_df['Model'].unique()} models, and yields an average C-123 index = {best_c}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >>> GENERATE RESULTS OVER TEST FOR TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Test Set Dataframe\n",
    "df_test = build_dataframe(data_dir=RESULTS_DIR, test=True, criterion=\"c-index-123-avg\")\n",
    "\n",
    "# Reminde me of the selected Ensemble:\n",
    "print(\"The Selected Ensemble is composed of:\")\n",
    "for selected_ in best_ensemble_df[\"Model\"].unique():\n",
    "    print(f\" >> {selected_}\")\n",
    "\n",
    "# Retrieve these models from TEST set dataframe\n",
    "df_test_selected = df_test[df_test[\"Model\"].isin(best_ensemble_df[\"Model\"].unique())]\n",
    "\n",
    "if (best_ensemble_df[\"Model\"].unique() != df_test_selected[\"Model\"].unique()).all():\n",
    "    raise AssertionError(\"Couldn't find all the models in the TEST set dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the results:\n",
    "if aggregation_method == \"mean\":\n",
    "    pred_prob_1 = df_test_selected.groupby(by=\"PatientID\")[\"SurvProb1\"].mean()\n",
    "    pred_prob_2 = df_test_selected.groupby(by=\"PatientID\")[\"SurvProb2\"].mean()\n",
    "    pred_prob_3 = df_test_selected.groupby(by=\"PatientID\")[\"SurvProb3\"].mean()\n",
    "else:\n",
    "    raise ValueError(\"Are you sure? Mean aggregation seemed to be the best so far. PLease review\")\n",
    "\n",
    "pred_submit = pd.DataFrame(\n",
    "    {\n",
    "        \"Probability for 1 year\": pred_prob_1,\n",
    "        \"Probability for 2 year\": pred_prob_2,\n",
    "        \"Probability for 3 year\": pred_prob_3,\n",
    "    }\n",
    ")\n",
    "\n",
    "pred_submit.index = pred_submit.index.astype(int)\n",
    "pred_submit = pred_submit.sort_values(by=\"PatientID\", ascending=True)\n",
    "pred_submit.to_csv(\"ClassificationOutcome.csv\")\n",
    "\n",
    "# Plot the distributions\n",
    "prob_1, true_1, event_1 = aggregate_results(\n",
    "    df=best_ensemble_df, method=aggregation_method, task=\"SurvProb1\"\n",
    ")\n",
    "prob_2, true_2, event_2 = aggregate_results(\n",
    "    df=best_ensemble_df, method=aggregation_method, task=\"SurvProb2\"\n",
    ")\n",
    "prob_3, true_3, event_3 = aggregate_results(\n",
    "    df=best_ensemble_df, method=aggregation_method, task=\"SurvProb3\"\n",
    ")\n",
    "\n",
    "val_probs = [prob_1, prob_2, prob_3]\n",
    "test_probs = [pred_prob_1, pred_prob_2, pred_prob_3]\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "bins_ = np.arange(0, 1, 0.02)\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "\n",
    "    hist_val, _ = np.histogram(val_probs[i], bins=bins_)\n",
    "    hist_test, _ = np.histogram(test_probs[i], bins=bins_)\n",
    "\n",
    "    plt.step(bins_[1:], hist_val / np.sum(hist_val), \"r\", label=\"Validation\")\n",
    "    plt.step(bins_[1:], hist_test / np.sum(hist_test), \"b\", label=\"Test\")\n",
    "    plt.title(f\"TASK 2: Probability {i + 1} year\")\n",
    "    plt.xlabel(\"Probability\")\n",
    "    plt.ylabel(\"Normalized Counts\")\n",
    "    plt.xlim([0.5, 1])\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 SELECTION OF MODELS: ENSEMBLE OF N RANDOM MODELS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "num_samples = 10000\n",
    "TOP_N = 0\n",
    "\n",
    "best_c = 0\n",
    "best_ensemble_df = pd.DataFrame()\n",
    "\n",
    "for i in range(num_samples):\n",
    "\n",
    "    num_models = np.random.choice([1, 2, 3, 4], 1)\n",
    "    df_ensemble_c123 = ensemble(\n",
    "        df=df_val, top_n=TOP_N, criterion=criterion, fold=-1, num_random=num_models[0]\n",
    "    )\n",
    "\n",
    "    prob_1, true_1, event_1 = aggregate_results(\n",
    "        df=df_ensemble_c123, method=aggregation_method, task=\"SurvProb1\"\n",
    "    )\n",
    "    prob_2, true_2, event_2 = aggregate_results(\n",
    "        df=df_ensemble_c123, method=aggregation_method, task=\"SurvProb2\"\n",
    "    )\n",
    "    prob_3, true_3, event_3 = aggregate_results(\n",
    "        df=df_ensemble_c123, method=aggregation_method, task=\"SurvProb3\"\n",
    "    )\n",
    "\n",
    "    # Sanity checks\n",
    "    assert (true_1 == true_2).all()\n",
    "    assert (event_1 == event_2).all()\n",
    "    assert (true_1 == true_3).all()\n",
    "    assert (event_1 == event_3).all()\n",
    "\n",
    "    # Compute C-indices\n",
    "    c_1, c_2, c_3 = compute_c_indices(\n",
    "        dura=true_1.to_list(),\n",
    "        ev=event_1.to_list(),\n",
    "        p1=prob_1.to_list(),\n",
    "        p2=prob_2.to_list(),\n",
    "        p3=prob_3.to_list(),\n",
    "    )\n",
    "\n",
    "    c_123 = (c_1 + c_2 + c_3) / 3\n",
    "\n",
    "    if c_123 > best_c:\n",
    "        print(f\"Found higher C-Index-123: {c_123: 1.3f}, sample = {i}/{num_samples}\")\n",
    "        best_c = c_123\n",
    "        best_ensemble_df = df_ensemble_c123.copy()\n",
    "\n",
    "# The best ensemble:\n",
    "print(\n",
    "    f\"The best ensemble found for {criterion} consists of the {best_ensemble_df['Model'].unique()} models, and yields an average C-123 index = {best_c}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >>> GENERATE RESULTS OVER TEST, FOR TASK 2: RISKY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Test Set Dataframe\n",
    "df_test = build_dataframe(data_dir=RESULTS_DIR, test=True, criterion=\"c-index-123-avg\")\n",
    "\n",
    "# Remind me of the selected Ensemble:\n",
    "print(\"The Selected Ensemble is composed of:\")\n",
    "for selected_ in best_ensemble_df[\"Model\"].unique():\n",
    "    print(f\" >> {selected_}\")\n",
    "\n",
    "# Retrieve these models from TEST set dataframe\n",
    "df_test_selected = df_test[df_test[\"Model\"].isin(best_ensemble_df[\"Model\"].unique())]\n",
    "\n",
    "if (best_ensemble_df[\"Model\"].unique() != df_test_selected[\"Model\"].unique()).all():\n",
    "    raise AssertionError(\"Couldn't find all the models in the TEST set dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the results:\n",
    "if aggregation_method == \"mean\":\n",
    "    pred_prob_1 = df_test_selected.groupby(by=\"PatientID\")[\"SurvProb1\"].mean()\n",
    "    pred_prob_2 = df_test_selected.groupby(by=\"PatientID\")[\"SurvProb2\"].mean()\n",
    "    pred_prob_3 = df_test_selected.groupby(by=\"PatientID\")[\"SurvProb3\"].mean()\n",
    "else:\n",
    "    raise ValueError(\"Are you sure? Mean aggregation seemed to be the best so far. PLease review\")\n",
    "\n",
    "pred_submit = pd.DataFrame(\n",
    "    {\n",
    "        \"Probability for 1 year\": pred_prob_1,\n",
    "        \"Probability for 2 year\": pred_prob_2,\n",
    "        \"Probability for 3 year\": pred_prob_3,\n",
    "    }\n",
    ")\n",
    "\n",
    "pred_submit.index = pred_submit.index.astype(int)\n",
    "pred_submit = pred_submit.sort_values(by=\"PatientID\", ascending=True)\n",
    "pred_submit.to_csv(\"ClassificationOutcomeRISKY.csv\")\n",
    "\n",
    "# Plot the distributions\n",
    "prob_1, true_1, event_1 = aggregate_results(\n",
    "    df=best_ensemble_df, method=aggregation_method, task=\"SurvProb1\"\n",
    ")\n",
    "prob_2, true_2, event_2 = aggregate_results(\n",
    "    df=best_ensemble_df, method=aggregation_method, task=\"SurvProb2\"\n",
    ")\n",
    "prob_3, true_3, event_3 = aggregate_results(\n",
    "    df=best_ensemble_df, method=aggregation_method, task=\"SurvProb3\"\n",
    ")\n",
    "\n",
    "val_probs = [prob_1, prob_2, prob_3]\n",
    "test_probs = [pred_prob_1, pred_prob_2, pred_prob_3]\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "bins_ = np.arange(0, 1, 0.02)\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "\n",
    "    hist_val, _ = np.histogram(val_probs[i], bins=bins_)\n",
    "    hist_test, _ = np.histogram(test_probs[i], bins=bins_)\n",
    "\n",
    "    plt.step(bins_[1:], hist_val / np.sum(hist_val), \"r\", label=\"Validation\")\n",
    "    plt.step(bins_[1:], hist_test / np.sum(hist_test), \"b\", label=\"Test\")\n",
    "    plt.title(f\"TASK 2: Probability {i + 1} year - RISKY\")\n",
    "    plt.xlabel(\"Probability\")\n",
    "    plt.ylabel(\"Normalized Counts\")\n",
    "    plt.xlim([0.4, 1])\n",
    "    plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".os",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
